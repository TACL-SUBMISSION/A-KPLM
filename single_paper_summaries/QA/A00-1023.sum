examples of using nlp and ie in question answering include shallow parsing [Kupiec 1993] TARGETANCHOR, deep parsing [Li et al 2002] [Litkowski 1999] [Voorhees 1999], and ie [Abney et al, 2000]. 
assuming that it is very likely that the answer is a named entity, TARGETANCHOR describes a ne-supported qa system that functions quite well when the expected answer type is one of the categories covered by the ne recognizer. 
in response, factoid question answering systems have evolved into two types: use-knowledge: extract query words from the input question, perform ir against the source corpus, possibly segment resulting documents, identify a set of segments containing likely answers, apply a set of heuristics that each consults a different source of knowledge to score each candidate, rank them, and select the best (Harabagiu et al, 2001; Hovy et al, 2001; TARGETANCHOR; Abney et al, 2000). 
it is worth noticing that in our experiment, the structural support used for answer-point identification only checks the binary links involving the asking point and the candidate answer points, instead of full template matching as proposed in TARGETANCHOR. 
qa is different than search engines in two aspects: (i) instead of a string of keyword search terms, the query is a natural language question, necessitating question parsing, (ii) instead of a list of documents or urls, a list of candidate answers at phrase level or sentence level are expected to be returned in response to a query, hence the need for text processing beyond keyword indexing, typically supported by natural language processing (nlp) and information extraction (ie) (Chinchor and marsh 1998, Hovy, hermjakob and lin 2001, TARGETANCHOR). 
