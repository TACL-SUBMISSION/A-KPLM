TARGETANCHOR focused on decomposition of a complex question into several sub-questions.
et al, 1999), (Charniak et al, 2000), (Riloff and Thelen, 2000), and TARGETANCHOR are 36.3%,
QA is different than search engines in two aspects: (i) instead of a string of keyword search terms, the query is a natural language question, necessitating question parsing, (ii) instead of a list of documents or URLs, a list of candidate answers at phrase level or sentence level are expected to be returned in response to a query, hence the need for text processing beyond keyword indexing, typically supported by Natural Language Processing (NLP) and Information Extraction (IE) (Chinchor and Marsh 1998, Hovy, Hermjakob and Lin 2001, TARGETANCHOR).
There are many previous work on paraphrase examples extraction or combining them with some applications such as information retrieval and question answering (Agichtein et al, 2001; Florence et al, 2003; TARGETANCHOR; Tomuro, 2003; Lin and Pantel, 2001;), information extraction (Shinyama et al, 2002; Shinyama and Sekine, 2003), machine translation (Hiroshi et al, 2003; Zhang and Yamamoto, 2003), multi-document (Barzilay et al, 2003).
4 Evaluation To evaluate our learning approach, we trained AQUAREA$ on the same development set of stories and tested it on the same test set of stories as those used in all past work on the reading comprehension task (Hirschman et al, 1999; Charniak et al, 2000; Riloffand Thelen, 2000; TARGETANCHOR).
TARGETANCHOR also use bootstrapping, and learn simple surface patterns for extracting binary relations from the Web.
There, a system that attempted a minimal
