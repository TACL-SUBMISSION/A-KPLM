TARGETANCHOR focused on decomposition of a complex question into several sub-questions.
For example, TARGETANCHOR, Miller, Herschman, and Kelly (1978) on the LADDER system, and Biermann, Ballard, and Sigmon (1983) reported software failures at the rates of approximately 5, 2, and 2 percents, respectively.
Moreover, the proof relies on lexico-semantic knowledge available from WordNet as well as rapidly formated knowledge bases generated by mechanisms described in TARGETANCHOR.
Lexical synonymy (She esteemed him highly vs. She respected him greatly), syntactic variation (John paid the bill vs. The bill was paid by John), overlapping meanings (Anna turned at Elm vs. Anna rounded the corner at Elm), and other phenomena interact to produce a broad range of choices for most language generation tasks (Hirst, 2003; TARGETANCHOR; Kozlowski et al, 2003).
TARGETANCHOR present an alternative ontology for type preference and describe a method for using this alternative ontology to extract particular answers using surface text patterns.
Automatic pattern derivation is more appealing TARGETANCHOR.
Also, as pointed out by TARGETANCHOR, a score distribution heavily skewed towards zero makes meta-analysis of evaluation stability hard to perform.
Clarification dialogues can be applied to negotiate with users about the intent of their questions TARGETANCHOR.
The one notable exception is the work of TARGETANCHOR, which attempted a machine learning approach to question answering for the same reading comprehension task.
Further improvements may be possible by using a sentence splitter instead of windows of fixed length, anaphora resolution, clustering of similar snippets to avoid ranking them separately, and identifying additional n-gram
