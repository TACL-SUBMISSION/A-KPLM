TARGETANCHOR focused on decomposition of a complex question into several sub-questions.
There are many previous work on paraphrase examples extraction or combining them with some applications such as information retrieval and question answering (Agichtein et al, 2001; Florence et al, 2003; TARGETANCHOR; Tomuro, 2003; Lin and Pantel, 2001;), information extraction (Shinyama et al, 2002; Shinyama and Sekine, 2003), machine translation (Hiroshi et al, 2003; Zhang and Yamamoto, 2003), multi-document (Barzilay et al, 2003).
There, a system that attempted a minimal understanding of both the question and the answer candidates, by translating them into their logical forms and using an inference engine, achieved a notably higher score than any surface-based system (Moldavan et al, 2002; Harabagiu et al, 2003).
In addition, a number of researchers have built systems to take reading comprehension examinations designed to evaluate children's reading levels (Charniak et al, 2000; Hirschman et al, 1999; Ng et al, 2000; Riloff and Thelen, 2000; TARGETANCHOR).
QA is different than search engines in two aspects: (i) instead of a string of keyword search terms, the query is a natural language question, necessitating question parsing, (ii) instead of a list of documents or URLs, a list of candidate answers at phrase level or sentence level are expected to be returned in response to a query, hence the need for text processing beyond keyword indexing, typically supported by Natural Language Processing (NLP) and Information Extraction (IE) (Chinchor and Marsh 1998, Hovy, Hermjakob and Lin 2001, TARGETANCHOR).
3.1 Feature Representation Our feature representation was
