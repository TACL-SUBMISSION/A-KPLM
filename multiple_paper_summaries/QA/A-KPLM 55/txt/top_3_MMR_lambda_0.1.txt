TARGETANCHOR focused on decomposition of a complex question into several sub-questions.
In addition, a number of researchers have built systems to take reading comprehension examinations designed to evaluate children's reading levels (Charniak et al, 2000; Hirschman et al, 1999; Ng et al, 2000; Riloff and Thelen, 2000; TARGETANCHOR).
The so-called "definition" or "other" questions at recent TREC evaluations TARGETANCHOR serve as good examples: "good answers" to these questions include interesting "nuggets" about a particular person, organization, entity, or event.
Further improvements may be possible by using a sentence splitter instead of windows of fixed length, anaphora resolution, clustering of similar snippets to avoid ranking them separately, and identifying additional n-gram attributes by bootstrapping TARGETANCHOR.
Also, as pointed out by TARGETANCHOR, a score distribution heavily skewed towards zero makes meta-analysis of evaluation stability hard to perform.
For example, TARGETANCHOR, Miller, Herschman, and Kelly (1978) on the LADDER system, and Biermann, Ballard, and Sigmon (1983) reported software failures at the rates of approximately 5, 2, and 2 percents, respectively.
Moreover, the proof relies on lexico-semantic knowledge available from WordNet as well as rapidly formated knowledge bases generated by mechanisms described in TARGETANCHOR.
Automatic pattern derivation is more appealing TARGETANCHOR.
Quarc TARGETANCHOR utilizes manually generated rules that selects a sentence deemed to contain the answer based on a combination of syntactic similarity and semantic correspondence (i.e., semantic categories of nouns).
Current systems TARGETANCHOR already employ traditional information extraction and machine learning for extracting answers from relevant documents.
TARGETANCHOR summarizes experience with the Transformational
