<html>
<head><title>sentence_pooling_strategy_top_5_adaptive_kplm_diversification_strategy_mmr_MMR_scoring_core_multinomial_cosine_MMR_alpha_0.9_dirichlet_prior_1_vectorisation_strategy_CountVectorizer_first_sent_selection_strategy_centroid_250_trimmed</title></head>
<body bgcolor="white">
<a name="1">[1]</a> <a href="#1" id=1>In this year, CoNLL-X shared task TARGETANCHOR focuses on multilingual dependency parsing without taking the language-specific knowledge into account. </a>
<a name="2">[2]</a> <a href="#2" id=2>+, traces MLE [ME] d2c c2d − Table 2: Overview of parsing approaches taken by participating groups (identified by the first three letters of the first author): algorithm (Y&M: Yamada and Matsumoto (2003), ILP: Integer Linear Programming), vertical direction (irrelevant, mpf: most probable first, bottom-up-spans, bottom-up-trees), horizontal direc- tion (irrelevant, mpf: most probable first, forward, backward), search (optimal, approximate, incremental, best-first exhaustive, deterministic), labeling (interleaved, separate and 1st step, separate and 2nd step), non-projective (ps-pr: through pseudo-projective approach), learner (ME: Maximum Entropy; learners in brackets were explored but not used in the official submission), preprocessing (projectivize, d2c: dependen- cies to constituents), postprocessing (deprojectivize, c2d: constituents to dependencies), learner parameter optimization per language anon-projectivity through approximate search, used for some languages b20 averaged perceptrons combined into a Bayes Point Machine cintroduced a single POS tag “aux” for all Swedish auxiliary and model verbs dby having no projectivity constraint eselective projectivity constraint for Japanese fseveral approaches to non-projectivity gusing some FEATS components to create some finer-grained POSTAG values hreattachment rules for some types of non-projectivity ihead automaton grammar jdetermined the maximally allowed distance for relations kthrough special parser actions lpseudo-projectivizing training data only mGreedy Prepend Algorithm nbut two separate learners used for unlabeled parsing versus labeling oboth foward and backward, then combined into a single tree with CLE pbut two separate SVMs used for unlabeled parsing versus labeling qforward parsing for Japanese and </a>
</body>
</html>